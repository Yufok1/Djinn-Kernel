# rebuild/load_massive_semantic_cache.py
"""
Load the MASSIVE semantic foundation cache (147k+ words)
This will take a long time but only needs to be done once!
"""

import time
from datetime import datetime

def load_massive_semantic_cache():
    """Load the massive semantic foundation and make cache permanent"""
    print("ğŸš€ LOADING MASSIVE SEMANTIC FOUNDATION CACHE")
    print("=" * 60)
    print("âš ï¸  WARNING: This will load ALL 147,000+ words from semantic libraries")
    print("âš ï¸  This may take several hours - but will only happen ONCE!")
    print("=" * 60)
    
    # Setup mock dependencies
    class MockComponent:
        def __init__(self):
            pass
        def __getattr__(self, name):
            return lambda *args, **kwargs: None
    
    print("ğŸ“š Creating LocalSemanticDatabase...")
    start_time = time.time()
    
    # Import and create the database (will load from cache or build from scratch)
    from local_semantic_database import LocalSemanticDatabase
    
    db = LocalSemanticDatabase(
        MockComponent(), MockComponent(), MockComponent(),
        MockComponent(), MockComponent(), MockComponent()
    )
    
    load_time = time.time() - start_time
    print(f"â±ï¸  Load completed in {load_time:.2f} seconds")
    print(f"ğŸ“Š References loaded: {len(db.semantic_references)}")
    print(f"ğŸ”— Relationships loaded: {len(db.semantic_relationships)}")
    
    # Make the cache permanent so it never expires
    print("\nğŸ”’ Making cache permanent...")
    db.make_cache_permanent()
    
    # Verify the cache is now permanent
    import json
    import os
    
    metadata_file = "semantic_cache/cache_metadata.json"
    if os.path.exists(metadata_file):
        with open(metadata_file, 'r') as f:
            metadata = json.load(f)
        
        if metadata.get('permanent_cache', False):
            print("âœ… Cache is now PERMANENT - will never expire!")
            print(f"ğŸ“… Cache expires: {metadata.get('cache_expires', 'Never')}")
        else:
            print("âŒ Failed to make cache permanent")
    
    print(f"\nğŸ‰ MASSIVE SEMANTIC CACHE LOADED AND MADE PERMANENT!")
    print(f"ğŸ“Š Total references: {len(db.semantic_references)}")
    print(f"ğŸ”— Total relationships: {len(db.semantic_relationships)}")
    print(f"â±ï¸  Load time: {load_time:.2f} seconds")
    print(f"ğŸ”’ Cache status: PERMANENT (never expires)")
    
    return {
        'references': len(db.semantic_references),
        'relationships': len(db.semantic_relationships),
        'load_time': load_time,
        'cache_permanent': True
    }

if __name__ == "__main__":
    results = load_massive_semantic_cache()
    
    print(f"\nğŸ¯ FINAL RESULTS:")
    print(f"   References: {results['references']:,}")
    print(f"   Relationships: {results['relationships']:,}")
    print(f"   Load time: {results['load_time']:.2f}s")
    print(f"   Cache permanent: {results['cache_permanent']}")
    
    if results['references'] > 100000:  # Should have 147k+ references
        print("ğŸ‰ SUCCESS! Massive semantic foundation loaded!")
        print("ğŸ’¾ Cache is permanent - you'll never need to load this again!")
    else:
        print("âš ï¸  Warning: Expected 147k+ references, got {results['references']:,}")
        print("   The cache may not have loaded the full dataset")
